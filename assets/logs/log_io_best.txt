2022-12-08 22:28:39.582503: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-08 22:28:40.924946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:40.946409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:40.946612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:40.958145: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-08 22:28:40.958606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:40.958782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:40.958918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:41.421073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:41.421264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:41.421407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:41.421526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20762 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
2022-12-08 22:28:41.952028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:41.952715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:41.952927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:41.953347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:41.953563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-08 22:28:41.967380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20762 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2022-12-08 22:28:42.036802: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
Epoch 1/25
2022-12-08 22:28:42.737544: W tensorflow/c/c_api.cc:349] Operation '{name:'lstm/while' id:199 op device:{requested: '', assigned: ''} def:{{{node lstm/while}} = While[T=[DT_INT32, DT_INT32, DT_INT32, DT_VARIANT, DT_FLOAT, 12032779882972063251, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=44, _read_only_resource_inputs=[8, 9, 10], _stateful_parallelism=false, body=lstm_while_body_101[], cond=lstm_while_cond_100[], output_shapes=[[], [], [], [], [?,96], 17584562593488234240, [], [], [], [], []], parallel_iterations=32](lstm/while/loop_counter, lstm/while/maximum_iterations, lstm/time, lstm/TensorArrayV2_1, lstm/zeros, lstm/zeros_1, lstm/strided_slice_1, lstm/TensorArrayUnstack/TensorListFromTensor, lstm/lstm_cell/kernel, lstm/lstm_cell/recurrent_kernel, lstm/lstm_cell/bias, lstm/while/EmptyTensorList, lstm/while/EmptyTensorList_1, lstm/while/EmptyTensorList_2, lstm/while/EmptyTensorList_3, lstm/while/EmptyTensorList_4, lstm/while/EmptyTensorList_5, lstm/while/EmptyTensorList_6, lstm/while/EmptyTensorList_7, lstm/while/EmptyTensorList_8, lstm/while/EmptyTensorList_9, lstm/while/EmptyTensorList_10, lstm/while/EmptyTensorList_11, lstm/while/EmptyTensorList_12, lstm/while/EmptyTensorList_13, lstm/while/EmptyTensorList_14, lstm/while/EmptyTensorList_15, lstm/while/EmptyTensorList_16, lstm/while/EmptyTensorList_17, lstm/while/EmptyTensorList_18, lstm/while/EmptyTensorList_19, lstm/while/EmptyTensorList_20, lstm/while/EmptyTensorList_21, lstm/while/EmptyTensorList_22, lstm/while/EmptyTensorList_23, lstm/while/EmptyTensorList_24, lstm/while/EmptyTensorList_25, lstm/while/EmptyTensorList_26, lstm/while/EmptyTensorList_27, lstm/while/EmptyTensorList_28, lstm/while/EmptyTensorList_29, lstm/while/EmptyTensorList_30, lstm/while/EmptyTensorList_31, lstm/while/EmptyTensorList_32)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2022-12-08 22:28:44.038077: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
/home/ubuntu/anaconda3/envs/cs-230-project/lib/python3.9/site-packages/keras/engine/training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
3300/3300 - 186s - loss: 0.0363 - mean_squared_error: 0.0080 - root_mean_squared_error: 0.0897 - val_loss: 0.0324 - val_mean_squared_error: 0.0026 - val_root_mean_squared_error: 0.0511 - 186s/epoch - 57ms/step
Epoch 2/25
3300/3300 - 184s - loss: 0.0299 - mean_squared_error: 0.0016 - root_mean_squared_error: 0.0395 - val_loss: 0.0307 - val_mean_squared_error: 9.1469e-04 - val_root_mean_squared_error: 0.0302 - 184s/epoch - 56ms/step
Epoch 3/25
3300/3300 - 184s - loss: 0.0282 - mean_squared_error: 6.1172e-04 - root_mean_squared_error: 0.0247 - val_loss: 0.0299 - val_mean_squared_error: 4.2471e-04 - val_root_mean_squared_error: 0.0206 - 184s/epoch - 56ms/step
Epoch 4/25
3300/3300 - 184s - loss: 0.0272 - mean_squared_error: 7.9287e-04 - root_mean_squared_error: 0.0282 - val_loss: 0.0273 - val_mean_squared_error: 0.0016 - val_root_mean_squared_error: 0.0404 - 184s/epoch - 56ms/step
Epoch 5/25
3300/3300 - 184s - loss: 0.0244 - mean_squared_error: 0.0018 - root_mean_squared_error: 0.0428 - val_loss: 0.0254 - val_mean_squared_error: 0.0024 - val_root_mean_squared_error: 0.0486 - 184s/epoch - 56ms/step
Epoch 6/25
3300/3300 - 184s - loss: 0.0243 - mean_squared_error: 0.0025 - root_mean_squared_error: 0.0496 - val_loss: 0.0250 - val_mean_squared_error: 0.0029 - val_root_mean_squared_error: 0.0535 - 184s/epoch - 56ms/step
Epoch 7/25
3300/3300 - 184s - loss: 0.0237 - mean_squared_error: 0.0028 - root_mean_squared_error: 0.0532 - val_loss: 0.0244 - val_mean_squared_error: 0.0031 - val_root_mean_squared_error: 0.0559 - 184s/epoch - 56ms/step
Epoch 8/25
3300/3300 - 184s - loss: 0.0229 - mean_squared_error: 0.0029 - root_mean_squared_error: 0.0540 - val_loss: 0.0236 - val_mean_squared_error: 0.0032 - val_root_mean_squared_error: 0.0567 - 184s/epoch - 56ms/step
Epoch 9/25
3300/3300 - 185s - loss: 0.0225 - mean_squared_error: 0.0030 - root_mean_squared_error: 0.0548 - val_loss: 0.0239 - val_mean_squared_error: 0.0034 - val_root_mean_squared_error: 0.0580 - 185s/epoch - 56ms/step
Epoch 10/25
3300/3300 - 184s - loss: 0.0221 - mean_squared_error: 0.0030 - root_mean_squared_error: 0.0549 - val_loss: 0.0234 - val_mean_squared_error: 0.0033 - val_root_mean_squared_error: 0.0578 - 184s/epoch - 56ms/step
Epoch 11/25
3300/3300 - 185s - loss: 0.0228 - mean_squared_error: 0.0032 - root_mean_squared_error: 0.0567 - val_loss: 0.0239 - val_mean_squared_error: 0.0035 - val_root_mean_squared_error: 0.0594 - 185s/epoch - 56ms/step
Epoch 12/25
3300/3300 - 185s - loss: 0.0229 - mean_squared_error: 0.0033 - root_mean_squared_error: 0.0573 - val_loss: 0.0234 - val_mean_squared_error: 0.0035 - val_root_mean_squared_error: 0.0588 - 185s/epoch - 56ms/step
Epoch 13/25
Restoring model weights from the end of the best epoch: 10.
3300/3300 - 185s - loss: 0.0223 - mean_squared_error: 0.0032 - root_mean_squared_error: 0.0569 - val_loss: 0.0238 - val_mean_squared_error: 0.0036 - val_root_mean_squared_error: 0.0600 - 185s/epoch - 56ms/step
Epoch 13: early stopping
Traceback (most recent call last):
  File "/home/ubuntu/cs230-project/trainLSTM.py", line 303, in <module>
    plotLossOverEpochs(history.history,'training_'+loss_f+'_'+str(nHLayers)+'_'+str(nHUnits)+'_'+str(learning_r))
  File "/home/ubuntu/cs230-project/utilities.py", line 73, in plotLossOverEpochs
    plt.savefig(fig_name)
  File "/home/ubuntu/anaconda3/envs/cs-230-project/lib/python3.9/site-packages/matplotlib/pyplot.py", line 954, in savefig
    res = fig.savefig(*args, **kwargs)
  File "/home/ubuntu/anaconda3/envs/cs-230-project/lib/python3.9/site-packages/matplotlib/figure.py", line 3274, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File "/home/ubuntu/anaconda3/envs/cs-230-project/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 2285, in print_figure
    with cbook._setattr_cm(self, manager=None), \
  File "/home/ubuntu/anaconda3/envs/cs-230-project/lib/python3.9/contextlib.py", line 119, in __enter__
    return next(self.gen)
  File "/home/ubuntu/anaconda3/envs/cs-230-project/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 2192, in _switch_canvas_and_return_print_method
    raise ValueError(
ValueError: Format '8006639526316317e-05' is not supported (supported formats: eps, jpeg, jpg, pdf, pgf, png, ps, raw, rgba, svg, svgz, tif, tiff, webp)
2022-12-09 03:21:44.297589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-09 03:21:46.466865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:46.495236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:46.495894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:46.513605: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-09 03:21:46.514629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:46.515118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:46.515467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:47.197716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:47.198215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:47.198552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:47.198825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 324 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
2022-12-09 03:21:48.091983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:48.092509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:48.102858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:48.114123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:48.114828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-09 03:21:48.115228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 324 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2022-12-09 03:21:48.141949: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
Epoch 1/15
2022-12-09 03:21:49.339526: W tensorflow/c/c_api.cc:349] Operation '{name:'lstm/while' id:199 op device:{requested: '', assigned: ''} def:{{{node lstm/while}} = While[T=[DT_INT32, DT_INT32, DT_INT32, DT_VARIANT, DT_FLOAT, 12032779882972063251, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=44, _read_only_resource_inputs=[8, 9, 10], _stateful_parallelism=false, body=lstm_while_body_101[], cond=lstm_while_cond_100[], output_shapes=[[], [], [], [], [?,96], 17584562593488234240, [], [], [], [], []], parallel_iterations=32](lstm/while/loop_counter, lstm/while/maximum_iterations, lstm/time, lstm/TensorArrayV2_1, lstm/zeros, lstm/zeros_1, lstm/strided_slice_1, lstm/TensorArrayUnstack/TensorListFromTensor, lstm/lstm_cell/kernel, lstm/lstm_cell/recurrent_kernel, lstm/lstm_cell/bias, lstm/while/EmptyTensorList, lstm/while/EmptyTensorList_1, lstm/while/EmptyTensorList_2, lstm/while/EmptyTensorList_3, lstm/while/EmptyTensorList_4, lstm/while/EmptyTensorList_5, lstm/while/EmptyTensorList_6, lstm/while/EmptyTensorList_7, lstm/while/EmptyTensorList_8, lstm/while/EmptyTensorList_9, lstm/while/EmptyTensorList_10, lstm/while/EmptyTensorList_11, lstm/while/EmptyTensorList_12, lstm/while/EmptyTensorList_13, lstm/while/EmptyTensorList_14, lstm/while/EmptyTensorList_15, lstm/while/EmptyTensorList_16, lstm/while/EmptyTensorList_17, lstm/while/EmptyTensorList_18, lstm/while/EmptyTensorList_19, lstm/while/EmptyTensorList_20, lstm/while/EmptyTensorList_21, lstm/while/EmptyTensorList_22, lstm/while/EmptyTensorList_23, lstm/while/EmptyTensorList_24, lstm/while/EmptyTensorList_25, lstm/while/EmptyTensorList_26, lstm/while/EmptyTensorList_27, lstm/while/EmptyTensorList_28, lstm/while/EmptyTensorList_29, lstm/while/EmptyTensorList_30, lstm/while/EmptyTensorList_31, lstm/while/EmptyTensorList_32)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2022-12-09 03:21:51.819564: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
/home/ubuntu/anaconda3/envs/cs-230-project/lib/python3.9/site-packages/keras/engine/training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
3300/3300 - 358s - loss: 0.0362 - mean_squared_error: 0.0081 - root_mean_squared_error: 0.0902 - val_loss: 0.0326 - val_mean_squared_error: 0.0025 - val_root_mean_squared_error: 0.0505 - 358s/epoch - 108ms/step
Epoch 2/15
3300/3300 - 357s - loss: 0.0297 - mean_squared_error: 0.0015 - root_mean_squared_error: 0.0392 - val_loss: 0.0304 - val_mean_squared_error: 9.3129e-04 - val_root_mean_squared_error: 0.0305 - 357s/epoch - 108ms/step
Epoch 3/15
3300/3300 - 354s - loss: 0.0281 - mean_squared_error: 6.2000e-04 - root_mean_squared_error: 0.0249 - val_loss: 0.0266 - val_mean_squared_error: 4.6148e-04 - val_root_mean_squared_error: 0.0215 - 354s/epoch - 107ms/step
Epoch 4/15
3300/3300 - 349s - loss: 0.0264 - mean_squared_error: 0.0011 - root_mean_squared_error: 0.0336 - val_loss: 0.0246 - val_mean_squared_error: 0.0016 - val_root_mean_squared_error: 0.0400 - 349s/epoch - 106ms/step
Epoch 5/15
3300/3300 - 355s - loss: 0.0248 - mean_squared_error: 0.0019 - root_mean_squared_error: 0.0437 - val_loss: 0.0245 - val_mean_squared_error: 0.0023 - val_root_mean_squared_error: 0.0478 - 355s/epoch - 108ms/step
Epoch 6/15
3300/3300 - 353s - loss: 0.0246 - mean_squared_error: 0.0025 - root_mean_squared_error: 0.0500 - val_loss: 0.0219 - val_mean_squared_error: 0.0024 - val_root_mean_squared_error: 0.0488 - 353s/epoch - 107ms/step
Epoch 7/15
3300/3300 - 355s - loss: 0.0236 - mean_squared_error: 0.0027 - root_mean_squared_error: 0.0523 - val_loss: 0.0225 - val_mean_squared_error: 0.0028 - val_root_mean_squared_error: 0.0528 - 355s/epoch - 107ms/step
Epoch 8/15
3300/3300 - 356s - loss: 0.0238 - mean_squared_error: 0.0030 - root_mean_squared_error: 0.0551 - val_loss: 0.0222 - val_mean_squared_error: 0.0029 - val_root_mean_squared_error: 0.0537 - 356s/epoch - 108ms/step
Epoch 9/15
3300/3300 - 301s - loss: 0.0226 - mean_squared_error: 0.0030 - root_mean_squared_error: 0.0547 - val_loss: 0.0210 - val_mean_squared_error: 0.0028 - val_root_mean_squared_error: 0.0526 - 301s/epoch - 91ms/step
Epoch 10/15
3300/3300 - 355s - loss: 0.0223 - mean_squared_error: 0.0030 - root_mean_squared_error: 0.0548 - val_loss: 0.0202 - val_mean_squared_error: 0.0027 - val_root_mean_squared_error: 0.0524 - 355s/epoch - 108ms/step
Epoch 11/15
3300/3300 - 356s - loss: 0.0222 - mean_squared_error: 0.0031 - root_mean_squared_error: 0.0554 - val_loss: 0.0223 - val_mean_squared_error: 0.0032 - val_root_mean_squared_error: 0.0565 - 356s/epoch - 108ms/step
Epoch 12/15
3300/3300 - 356s - loss: 0.0222 - mean_squared_error: 0.0031 - root_mean_squared_error: 0.0558 - val_loss: 0.0217 - val_mean_squared_error: 0.0031 - val_root_mean_squared_error: 0.0560 - 356s/epoch - 108ms/step
Epoch 13/15
Restoring model weights from the end of the best epoch: 10.
3300/3300 - 357s - loss: 0.0224 - mean_squared_error: 0.0032 - root_mean_squared_error: 0.0567 - val_loss: 0.0214 - val_mean_squared_error: 0.0031 - val_root_mean_squared_error: 0.0554 - 357s/epoch - 108ms/step
Epoch 13: early stopping
